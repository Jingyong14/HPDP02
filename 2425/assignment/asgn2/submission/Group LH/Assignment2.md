<a href="https://github.com/drshahizan/HPDP/stargazers"><img src="https://img.shields.io/github/stars/drshahizan/HPDP" alt="Stars Badge"/></a>
<a href="https://github.com/drshahizan/HPDP/network/members"><img src="https://img.shields.io/github/forks/drshahizan/HPDP" alt="Forks Badge"/></a>
<a href="https://github.com/drshahizan/HPDP/pulls"><img src="https://img.shields.io/github/issues-pr/drshahizan/HPDP" alt="Pull Requests Badge"/></a>
<a href="https://github.com/drshahizan/HPDP/issues"><img src="https://img.shields.io/github/issues/drshahizan/HPDP" alt="Issues Badge"/></a>
<a href="https://github.com/drshahizan/HPDP/graphs/contributors"><img alt="GitHub contributors" src="https://img.shields.io/github/contributors/drshahizan/HPDP?color=2b9348"></a>
![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdrshahizan%2FHPDP&labelColor=%23d9e3f0&countColor=%23697689&style=flat)

# üìò Assignment 2: Mastering Big Data Handling

**Team Member**: Group LH
1. Muhammad Daniel Hakim Bin Syahrulnizam (A22EC0207)
2. Muhammad Luqman Hakim Bin Mohd Rizaudin (A22EC0086)

## üìù Assignment Tasks

### Task 1: Dataset Selection

* Choose a dataset **larger than 700MB** from a reliable source (e.g., Kaggle, UCI Repository).
* The dataset should be rich enough for exploratory and performance comparison.
* Mention dataset details (source, size, domain, number of records).

### Task 2: Load and Inspect Data

* Use **Python** (preferably on **Google Colab**) to load the dataset.
* Ensure the loading method accommodates the file size efficiently.
* Include a brief inspection (e.g., shape, column names, datatypes).

### Task 3: Apply Big Data Handling Strategies

Apply the following **five strategies** to your dataset:

1. **Load Less Data**

   * Load only required columns or filter relevant rows during read operation.

2. **Use Chunking**

   * Process the data in small chunks using `pandas.read_csv(chunksize=...)`.

3. **Optimize Data Types**

   * Convert columns to appropriate types (e.g., `category`, `float32`) to reduce memory usage.

4. **Sampling**

   * Apply random or stratified sampling to reduce the dataset size for fast prototyping.

5. **Parallel Processing with Dask**

   * Use **Dask DataFrame** to read and process large files in parallel.

Each strategy must be:

* Clearly **explained**
* **Implemented with code snippets**
* Accompanied by **screenshots of outputs** or **results**

### Task 4: Comparative Analysis

Perform a **comparison** between:

* **Traditional methods** (e.g., full Pandas load)
* **Optimized strategies** above

Measure and discuss:

* **Memory usage**
* **Execution time**
* **Ease of processing**

Use tables or charts to present your analysis if applicable.

### Task 5: Conclusion & Reflection

* Summarize key observations.
* Discuss the **benefits and limitations** of each method.
* Reflect on what was learned from this assignment.



## Contribution üõ†Ô∏è
Please create an [Issue](https://github.com/drshahizan/HPDP/issues) for any improvements, suggestions or errors in the content.

You can also contact me using [Linkedin](https://www.linkedin.com/in/drshahizan/) for any other queries or feedback.

[![Visitors](https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fdrshahizan&labelColor=%23697689&countColor=%23555555&style=plastic)](https://visitorbadge.io/status?path=https%3A%2F%2Fgithub.com%2Fdrshahizan)
![](https://hit.yhype.me/github/profile?user_id=81284918)


