## 👥 Group Members

- Wong Qiao Ying (A22EC0118)
- Nik Zulaikhaa Binti Zuraidi Afandi (A22EC0232)

---
## Introduction
In the modern data-driven world, organizations face the challenge of managing and extracting insights from massive datasets that far exceed the capabilities of traditional data handling tools. This assignment provides hands-on experience in managing such datasets using Python and its scalable libraries.

In this assignment, a dataset with 1GB size will be used to performed a few optimized strategies including:
- Load Less Data
- Use Chunking
- Optimize Data Types
- Sampling
- Parallel Processing with Dask

## 📂 Project Resources

- 📊 **Dataset**: [900K Spotify Dataset on Kaggle](https://www.kaggle.com/datasets/devdope/900k-spotify)
- 💻 **Google Colab Notebook**: [Colab Link](https://colab.research.google.com/drive/1oj236d2rKkwgNYk3KVFs2wI9bzTgKte9?usp=sharing)
- 📝 **Full Report**: [Report Link](https://github.com/Jingyong14/HPDP02/blob/main/2425/assignment/asgn2/submission/Group_Colab/big_data.md)
- 📘 **Assignment Logbook**: [Google Docs Logbook](https://docs.google.com/document/d/1kHPzdrlHnGcFiWhbS9kZ4wsZAI8TMlyzq94FyRQA110/edit?usp=sharing)


