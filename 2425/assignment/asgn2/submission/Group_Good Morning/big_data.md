# Assignment 2: Mastering Big Data Handling

## Group Name: Good Morning

| Name                       | Matric No   |
|----------------------------|-------------|
| Neo Zheng Weng             | A22EC0093   |
| Wong Khai Shian Nicholas   | A22EC0292   |

---

## Table of Contents

- [Introduction](#introduction)
- [Objectives](#objectives)
- [Task 1: Dataset Selection](#task-1-dataset-selection)
- [Task 2: Load and Inspect Data](#task-2-load-and-inspect-data)
- [Task 3: Apply Big Data Handling Strategies](#task-3-apply-big-data-handling-strategies)
- [Task 4: Comparative Analysis](#task-4-comparative-analysis)
- [Task 5: Conclusion & Reflection](#task-5-conclusion--reflection)  

---

## Introduction

In today’s data-driven environment, analysts must process datasets that exceed traditional memory constraints. This project utilizes the 2019 Airline Delays and Cancellations dataset (1.37 GB) from Kaggle to demonstrate scalable data loading and processing. Using Python with Pandas, Dask, and Polars, we apply selective column loading, chunked reading, data‐type optimization, sampling, and parallel execution to handle big data processing. We evaluate each library’s performance by measuring execution time and DataFrame‐only memory usage, thereby identifying efficient approaches for large‐volume flight analytics.

## Objectives

The objectives of this assignment are:

- To handle data volumes above 700MB.
- To apply big data handling strategies, including chunking, sampling, type optimization, and parallel computing.
- To evaluate and compare the performance between traditional and optimized data handling methods based on execution time and memory usage.

## Task 1: Dataset Selection

## Task 2: Load and Inspect Data

## Task 3: Apply Big Data Handling Strategies

## Task 4: Comparative Analysis

## Task 5: Conclusion & Reflection
