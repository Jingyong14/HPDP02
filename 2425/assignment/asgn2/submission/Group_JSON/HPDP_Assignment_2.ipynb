{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HPDP Assignment 2: Big Data Processing\n",
        "\n",
        "This notebook demonstrates different strategies for handling large datasets using Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "# Set the path to the dataset files\n",
        "dataset_path = \"Traffic and Weather Datasets/Traffic Datasets\"\n",
        "file_path4 = os.path.join(dataset_path, \"traffic_speed_PeMSD4.csv\")\n",
        "file_path7 = os.path.join(dataset_path, \"traffic_speed_PeMSD7.csv\")\n",
        "\n",
        "print(\"Path to dataset files:\")\n",
        "print(f\"File 1: {file_path4}\")\n",
        "print(f\"File 2: {file_path7}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def track_performance(method_name, start_time, start_memory, df):\n",
        "    \"\"\"Track performance metrics for data cleaning operations.\"\"\"\n",
        "    end_time = time.time()\n",
        "    end_memory = psutil.Process().memory_info().rss / (1024 * 1024)  # MB\n",
        "\n",
        "    time_taken = end_time - start_time\n",
        "    throughput = len(df) / time_taken if time_taken > 0 else 0\n",
        "    memory_used = end_memory - start_memory  # in MB\n",
        "\n",
        "    return {\n",
        "        \"Method\": method_name,\n",
        "        \"Time (s)\": time_taken,\n",
        "        \"Throughput (rows/s)\": throughput,\n",
        "        \"Memory Used (MB)\": memory_used\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "\n",
        "# Load the dataset using pandas directly with the local file path\n",
        "try:\n",
        "    chunk_size = 10000  # Process data in chunks of 10,000 rows\n",
        "    for df1 in pd.read_csv(file_path4, chunksize=chunk_size):\n",
        "       print(track_performance(\"Pandas\", start_time, start_memory, df1))\n",
        "\n",
        "    # Display the first few rows of the dataset\n",
        "    print(\"First 5 records:\")\n",
        "    print(df1.head())\n",
        "\n",
        "    # Describe the dataset\n",
        "    print(\"\\n--- Dataset Description ---\")\n",
        "    print(f\"Shape of the dataset: {df1.shape}\")\n",
        "    print(\"\\nData Types of Columns:\")\n",
        "    print(df1.dtypes)\n",
        "\n",
        "    pandas_result = track_performance(\"Pandas\", start_time, start_memory, df1)\n",
        "    print(pandas_result)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path4} or {file_path7}\")\n",
        "    print(\"Please check the contents of the directory:\", os.listdir(dataset_path))\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import dask.dataframe as dd\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    # Read the CSV files into Dask DataFrames\n",
        "    ddf4 = dd.read_csv(file_path4)\n",
        "    ddf5 = dd.read_csv(file_path7)\n",
        "\n",
        "    print(\"\\nDask DataFrame for traffic_speed_PeMSD4:\")\n",
        "    print(ddf4.info())\n",
        "    print(ddf4.head())\n",
        "\n",
        "    print(\"\\nDask DataFrame for traffic_speed_PeMSD7:\")\n",
        "    print(ddf5.info())\n",
        "    print(ddf5.head())\n",
        "\n",
        "    dask_result = track_performance(\"Dask\", start_time, start_memory, ddf4)\n",
        "    print(dask_result)\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: One or more files not found: {e}\")\n",
        "    print(f\"Please ensure the following paths are correct:\")\n",
        "    print(f\"- {file_path4}\")\n",
        "    print(f\"- {file_path7}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "start_time = time.time()\n",
        "start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "\n",
        "try:\n",
        "    # Read the CSV files into Polars DataFrames\n",
        "    lf4 = pl.scan_csv(file_path4)\n",
        "    lf5 = pl.scan_csv(file_path7)\n",
        "\n",
        "    print(\"\\nInspecting traffic_speed_PeMSD4.csv using Polars:\")\n",
        "    # Get the first 5 rows\n",
        "    head4 = lf4.limit(5).collect()\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(head4)\n",
        "\n",
        "    print(\"\\nPolars LazyFrame for traffic_speed_PeMSD4:\")\n",
        "    print(lf4)\n",
        "\n",
        "    print(\"\\nPolars LazyFrame for traffic_speed_PeMSD7:\")\n",
        "    print(lf5)\n",
        "\n",
        "    # Collect the LazyFrame to get a Polars DataFrame before getting its length\n",
        "    df4_collected = lf4.collect()\n",
        "    polar_result = track_performance(\"Polars\", start_time, start_memory, df4_collected)\n",
        "    print(polar_result)\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: One or more files not found: {e}\")\n",
        "    print(f\"Please ensure the following paths are correct:\")\n",
        "    print(f\"- {file_path4}\")\n",
        "    print(f\"- {file_path7}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_results(results):\n",
        "    \"\"\"Plot performance comparison results in 3 columns\"\"\"\n",
        "     # Convert results to pandas DataFrame for plotting\n",
        "    results_df = pl.DataFrame(results).to_pandas()\n",
        "\n",
        "    # Set plot style\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Define metrics and color palette\n",
        "    metrics = [\"Time (s)\", \"Throughput (rows/s)\", \"Memory Used (MB)\"]\n",
        "    palette = sns.color_palette(\"pastel\", n_colors=len(results_df['Method'].unique()))\n",
        "\n",
        "    # Create a single row of 3 subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axes[i]\n",
        "        sns.barplot(\n",
        "            x=\"Method\", y=metric, hue=\"Method\", legend=False,\n",
        "            data=results_df, palette=palette, ax=ax\n",
        "        )\n",
        "\n",
        "        # Add values on top of the bars\n",
        "        for p in ax.patches:\n",
        "            ax.annotate(f'{p.get_height():.2f}',\n",
        "                        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                        ha='center', va='center', fontsize=10, color='black',\n",
        "                        xytext=(0, 5), textcoords='offset points')\n",
        "\n",
        "        ax.set_title(f\"{metric} Comparison\", fontsize=14)\n",
        "        ax.set_xlabel(\"\")\n",
        "        ax.set_ylabel(metric)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Collect all results\n",
        "results = [polar_result, pandas_result]\n",
        "\n",
        "# Plot results\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Plot comparison between Dask and Pandas\n",
        "results = [dask_result, pandas_result]\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Plot comparison between Polars and Dask\n",
        "results = [polar_result, dask_result]\n",
        "plot_results(results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}