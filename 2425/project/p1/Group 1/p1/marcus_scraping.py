# -*- coding: utf-8 -*-
"""Marcus_Scraping

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_p-oHFxBtOxHTHRfq_3NSvKJyvsLQPN5
"""

!pip install httpx parsel

import httpx
import csv
import time
import json
from parsel import Selector

base_url = "https://www.carlist.my/cars-for-sale/malaysia?page_number={}&page_size=25"
start_page = 1
end_page = 1744
car_listings = []
output_file = 'carlist_data.csv'

# Extract from the <script type="application/ld+json"> block
def extract_json_ld(selector):
    scripts = selector.css('script[type="application/ld+json"]::text').getall()
    for script in scripts:
        try:
            data = json.loads(script)
            if isinstance(data, list):
                for d in data:
                    if 'itemListElement' in d:
                        return d['itemListElement']
        except json.JSONDecodeError:
            continue
    return None

# Get mileage from <i class="icon--meter">
def get_mileage(selector):
    meter_icon = selector.css('i.icon--meter')
    if meter_icon:
        return meter_icon.xpath('following-sibling::text()[1]').get(default='').strip()
    return ''

# Scraping function to get the page content
def fetch_page(url):
    response = httpx.get(url)
    return response.text

# Correct full location (e.g., Petaling Jaya, Selangor)
def get_location(selector, car):
    try:
        # Extract the locality and region from the car JSON data
        address_locality = car.get('seller', {}).get('homeLocation', {}).get('address', {}).get('addressLocality', '')
        address_region = car.get('seller', {}).get('homeLocation', {}).get('address', {}).get('addressRegion', '')

        # Combine locality and region to form the location
        if address_locality and address_region:
            location = f"{address_locality}, {address_region}"
        else:
            location = ', '.join([address_locality, address_region]).strip(', ')

        return location
    except Exception as e:
        return ''

# Correct sales agent (e.g., Sales Agent)
def get_sales_channel(selector):
    try:
        # Use CSS selector to find the sales agent text
        sales_info = selector.css('span.c-chip--icon::text').get(default='').strip()
        # Replace "Sales" with "Sales Agent"
        sales_channel = sales_info.replace("Sales", "Sales Agent") if "Sales" in sales_info else sales_info
        return sales_channel
    except Exception as e:
        return ''

# Start timing
start_time = time.time()

# Start scraping
for page in range(start_page, end_page + 1):
    url = base_url.format(page)
    print(f"\nüîé Scraping page {page}: {url}")

    response = httpx.get(url, headers={"User-Agent": "Mozilla/5.0"})
    selector = Selector(response.text)

    articles = selector.css('article.listing')
    json_ld = extract_json_ld(selector)

    if not json_ld:
        print("‚ö†Ô∏è No JSON-LD found.")
        continue

    for i, (article_sel, json_car) in enumerate(zip(articles, json_ld)):
        car = json_car['item']

        name = article_sel.attrib.get('data-title', '')
        brand = article_sel.attrib.get('data-make', '')
        model = article_sel.attrib.get('data-model', '')
        body = article_sel.attrib.get('data-body-type', '')
        transmission = article_sel.attrib.get('data-transmission', '')
        installment = article_sel.attrib.get('data-installment', '')

        year = car.get('vehicleModelDate', '')
        fuel = car.get('fuelType', '')
        color = car.get('color', '')
        price = car.get('offers', {}).get('price', '')
        condition = car.get('itemCondition', '')
        seats = car.get('seatingCapacity', '')

        mileage = get_mileage(article_sel)
        location = get_location(article_sel, car)
        sales_channel = get_sales_channel(article_sel)

        car_listings.append({
            'Car Name': name,
            'Car Brand': brand,
            'Car Model': model,
            'Manufacture Year': year,
            'Body Type': body,
            'Fuel Type': fuel,
            'Mileage': mileage,
            'Transmission': transmission,
            'Color': color,
            'Price': price,
            'Installment': installment,
            'Condition': condition,
            'Seating Capacity': seats,
            'Location': location,
            'Sales Channel': sales_channel
        })

    print(f"‚úÖ Extracted {len(articles)} listings from page {page}")
    time.sleep(2)  # Be kind to the server

# Save to CSV
if car_listings:
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=car_listings[0].keys())
        writer.writeheader()
        writer.writerows(car_listings)
    print(f"\nüíæ Saved {len(car_listings)} listings to '{output_file}'")
else:
    print("‚ö†Ô∏è No data extracted.")

# End timing
end_time = time.time()
print(f"\nüïí Total scraping time: {end_time - start_time:.2f} seconds")