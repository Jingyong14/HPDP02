{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2jcRc-vz_sO"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Range of pet IDs\n",
        "start_id = 2101\n",
        "end_id = 2200  # inclusive\n",
        "\n",
        "# Setup Chrome options (headless mode optional)\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "\n",
        "# Setup WebDriver\n",
        "service = Service()  # Adjust path to chromedriver if needed\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Write CSV header\n",
        "with open('pets_selenium.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "    csv_writer.writerow([\n",
        "        \"Pet ID\", \"Name\", \"Type\", \"Species\", \"Profile\", \"Amount\", \"Vaccinated\", \"Dewormed\",\n",
        "        \"Spayed\", \"Condition\", \"Body\", \"Color\", \"Location\", \"Posted\", \"Price\",\n",
        "        \"Uploader Type\", \"Uploader Name\", \"Status\"\n",
        "    ])\n",
        "\n",
        "    # Loop over each pet ID\n",
        "    for pet_id in range(start_id, end_id + 1):\n",
        "        url = f\"https://www.petfinder.my/pets/{pet_id}/\"\n",
        "        try:\n",
        "            driver.get(url)\n",
        "            time.sleep(1)  # Let the page load\n",
        "\n",
        "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "            # Extract pet name\n",
        "            pet_title_tag = soup.find('div', class_='pet_title')\n",
        "            pet_name = pet_title_tag.find('td', align=\"center\").text.strip() if pet_title_tag else \"N/A\"\n",
        "\n",
        "            # Extract pet details\n",
        "            info_table = soup.find('table', class_='pet_box')\n",
        "            pet_details = {}\n",
        "\n",
        "            if info_table:\n",
        "                rows = info_table.find_all('tr')\n",
        "                for row in rows:\n",
        "                    cols = row.find_all('td')\n",
        "                    if len(cols) >= 2:\n",
        "                        key_tag = cols[0].find('b')\n",
        "                        if key_tag:\n",
        "                            key = key_tag.text.strip().replace(\":\", \"\")\n",
        "                            value = cols[1].text.strip()\n",
        "                            pet_details[key] = value\n",
        "\n",
        "            # Type and Species\n",
        "            pet_type = next(iter(pet_details.keys()), \"N/A\")\n",
        "            pet_species = pet_details.get(pet_type, \"N/A\")\n",
        "\n",
        "            # Price/Adoption Fee\n",
        "            adoption_fee = \"N/A\"\n",
        "            for row in info_table.find_all('tr') if info_table else []:\n",
        "                cols = row.find_all('td')\n",
        "                if len(cols) >= 2:\n",
        "                    key_tag = cols[0].find('b')\n",
        "                    if key_tag and 'Adoption Fee' in key_tag.text:\n",
        "                        fee_tag = cols[1].find('b')\n",
        "                        adoption_fee = fee_tag.text.strip() if fee_tag else cols[1].text.strip()\n",
        "\n",
        "            # Uploader\n",
        "            uploader_td = soup.find('td', align=\"left\", width=\"130\", valign=\"middle\")\n",
        "            uploader_type = uploader_td.find('font').text.strip() if uploader_td and uploader_td.find('font') else \"N/A\"\n",
        "            uploader_name_tag = uploader_td.find('a', class_='darkgrey') if uploader_td else None\n",
        "            uploader_name = uploader_name_tag.text.strip() if uploader_name_tag else \"N/A\"\n",
        "\n",
        "            # Status\n",
        "            status_tag = soup.find('div', class_='pet_label')\n",
        "            pet_status = status_tag.text.strip() if status_tag else \"N/A\"\n",
        "\n",
        "            # Write to CSV\n",
        "            csv_writer.writerow([\n",
        "                pet_id,\n",
        "                pet_name,\n",
        "                pet_type,\n",
        "                pet_species,\n",
        "                pet_details.get('Profile', 'N/A'),\n",
        "                pet_details.get('Amount', 'N/A'),\n",
        "                pet_details.get('Vaccinated', 'N/A'),\n",
        "                pet_details.get('Dewormed', 'N/A'),\n",
        "                pet_details.get('Spayed', 'N/A'),\n",
        "                pet_details.get('Condition', 'N/A'),\n",
        "                pet_details.get('Body', 'N/A'),\n",
        "                pet_details.get('Color', 'N/A'),\n",
        "                pet_details.get('Location', 'N/A'),\n",
        "                pet_details.get('Posted', 'N/A'),\n",
        "                adoption_fee,\n",
        "                uploader_type,\n",
        "                uploader_name,\n",
        "                pet_status\n",
        "            ])\n",
        "            print(f\"Scraped pet ID: {pet_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape pet {pet_id}: {e}\")\n",
        "\n",
        "# Close WebDriver\n",
        "driver.quit()\n"
      ]
    }
  ]
}