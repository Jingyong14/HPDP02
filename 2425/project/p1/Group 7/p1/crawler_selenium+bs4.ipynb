{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NST News Scraper Notebook\n",
    "This notebook contains code for scraping news articles from the New Straits Times (NST) website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install selenium\n",
    "!pip install webdriver-manager\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. WebDriver Setup Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def setup_driver():\n",
    "    print(\"Setting up Chrome WebDriver...\")\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.7049.96 Safari/537.36\")\n",
    "        \n",
    "        # Get ChromeDriver with correct architecture\n",
    "        driver_path = ChromeDriverManager().install()\n",
    "        print(f\"ChromeDriver path: {driver_path}\")\n",
    "        \n",
    "        if not os.path.exists(driver_path):\n",
    "            raise Exception(f\"ChromeDriver not found at {driver_path}\")\n",
    "            \n",
    "        # Verify the ChromeDriver file\n",
    "        if not driver_path.endswith('.exe'):\n",
    "            raise Exception(f\"Invalid ChromeDriver file: {driver_path}\")\n",
    "            \n",
    "        service = Service(executable_path=driver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        # Test the browser connection\n",
    "        driver.get(\"about:blank\")\n",
    "        print(\"Chrome WebDriver setup successful!\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up Chrome WebDriver: {str(e)}\")\n",
    "        print(f\"Python version: {sys.version}\")\n",
    "        print(f\"Selenium version: {webdriver.__version__}\")\n",
    "        print(f\"Chrome version: 135.0.7049.96\")\n",
    "        print(f\"System architecture: {sys.platform}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Article Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def scrape_nst_articles(base_url, start_page, end_page):\n",
    "    driver = setup_driver()\n",
    "    all_articles_data = []\n",
    "    \n",
    "    try:\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            try:\n",
    "                # Construct URL for each page\n",
    "                url = f\"{base_url}?page={page}\"\n",
    "                print(f\"\\nScraping page {page}...\")\n",
    "                \n",
    "                print(f\"Accessing URL: {url}\")\n",
    "                driver.get(url)\n",
    "                \n",
    "                # Wait for the content to load\n",
    "                print(\"Waiting for content to load...\")\n",
    "                time.sleep(1)  # Initial wait\n",
    "                \n",
    "                # Wait for article elements to be present\n",
    "                try:\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CLASS_NAME, \"article-teaser\"))\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Timeout waiting for articles to load: {str(e)}\")\n",
    "                \n",
    "                # Get the page source after JavaScript has rendered\n",
    "                page_source = driver.page_source\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                \n",
    "                # Try different article selectors\n",
    "                article_selectors = [\n",
    "                    (\"div\", \"article-teaser\"),\n",
    "                    (\"div\", \"article-item\"),\n",
    "                    (\"article\", None),\n",
    "                    (\"div\", \"list-article\")\n",
    "                ]\n",
    "                \n",
    "                articles = []\n",
    "                for tag, class_name in article_selectors:\n",
    "                    if class_name:\n",
    "                        articles = soup.find_all(tag, class_=class_name)\n",
    "                    else:\n",
    "                        articles = soup.find_all(tag)\n",
    "                    \n",
    "                    print(f\"Found {len(articles)} articles with selector {tag}.{class_name if class_name else ''}\")\n",
    "                    if articles:\n",
    "                        break\n",
    "                \n",
    "                # Process each article\n",
    "                for i, article in enumerate(articles, 1):\n",
    "                    try:\n",
    "                        print(f\"\\nProcessing article {i} on page {page}:\")\n",
    "                        \n",
    "                        # Extract headline\n",
    "                        headline = \"\"\n",
    "                        headline_selectors = [\n",
    "                            (\"h2\", \"article-title\"),\n",
    "                            (\"h6\", None),\n",
    "                            (\"h2\", None),\n",
    "                            (\"h3\", None),\n",
    "                            (\"a\", \"article-link\")\n",
    "                        ]\n",
    "                        \n",
    "                        for tag, class_name in headline_selectors:\n",
    "                            elements = article.find_all(tag, class_=class_name) if class_name else article.find_all(tag)\n",
    "                            for element in elements:\n",
    "                                text = element.get_text(strip=True)\n",
    "                                if text and len(text) > 20:  # Likely a headline if longer than 20 chars\n",
    "                                    headline = text\n",
    "                                    break\n",
    "                            if headline:\n",
    "                                break\n",
    "                        \n",
    "                        # Skip empty articles\n",
    "                        if not headline:\n",
    "                            print(\"Skipping empty article\")\n",
    "                            continue\n",
    "                            \n",
    "                        print(f\"Headline: {headline}\")\n",
    "                        \n",
    "                        # Extract date and section\n",
    "                        date = \"\"\n",
    "                        section = \"Crime & Courts\"  # Default section\n",
    "                        date_selectors = [\n",
    "                            (\"time\", \"article-date\"),\n",
    "                            (\"span\", \"created-date\"),\n",
    "                            (\"div\", \"article-date\"),\n",
    "                            (\"span\", \"date\"),\n",
    "                            (\"div\", \"field-item\"),\n",
    "                            (\"div\", \"article-meta\")\n",
    "                        ]\n",
    "                        \n",
    "                        for tag, class_name in date_selectors:\n",
    "                            elements = article.find_all(tag, class_=class_name) if class_name else article.find_all(tag)\n",
    "                            for element in elements:\n",
    "                                date_text = element.get_text(strip=True)\n",
    "                                if any(pattern in date_text.lower() for pattern in ['2024', '2023', '2025']):\n",
    "                                    parts = date_text.split('@')\n",
    "                                    if len(parts) > 1:\n",
    "                                        section_part = parts[0].strip()\n",
    "                                        if any(category in section_part.lower() for category in ['politics', 'crime', 'nation', 'business', 'sports']):\n",
    "                                            section = section_part.split('Apr')[0].strip()\n",
    "                                            date = parts[0].replace(section, '').strip() + ' @' + parts[1].strip()\n",
    "                                        else:\n",
    "                                            date = date_text\n",
    "                                    else:\n",
    "                                        date = date_text\n",
    "                                    break\n",
    "                            if date:\n",
    "                                break\n",
    "                        \n",
    "                        print(f\"Section: {section}\")\n",
    "                        print(f\"Date: {date}\")\n",
    "                        \n",
    "                        # Extract summary\n",
    "                        summary = \"\"\n",
    "                        summary_selectors = [\n",
    "                            (\"div\", \"article-teaser\"),\n",
    "                            (\"div\", \"field-item\"),\n",
    "                            (\"p\", \"article-summary\"),\n",
    "                            (\"div\", \"summary\"),\n",
    "                            (\"p\", None)\n",
    "                        ]\n",
    "                        \n",
    "                        for tag, class_name in summary_selectors:\n",
    "                            elements = article.find_all(tag, class_=class_name) if class_name else article.find_all(tag)\n",
    "                            for element in elements:\n",
    "                                summary_text = element.get_text(strip=True)\n",
    "                                if len(summary_text) > 50 and not any(keyword in summary_text.lower() for keyword in ['read more', 'click here']):\n",
    "                                    if summary_text.startswith(('GEORGE TOWN:', 'IPOH:', 'TAPAH:', 'BALING:', 'ALOR STAR:')) or ':' in summary_text[:50]:\n",
    "                                        summary = summary_text\n",
    "                                        break\n",
    "                            if summary:\n",
    "                                break\n",
    "                        \n",
    "                        # If no summary found, try to get the first paragraph that looks like a summary\n",
    "                        if not summary:\n",
    "                            paragraphs = article.find_all('p')\n",
    "                            for p in paragraphs:\n",
    "                                text = p.get_text(strip=True)\n",
    "                                if len(text) > 50 and not any(keyword in text.lower() for keyword in ['read more', 'click here']):\n",
    "                                    summary = text\n",
    "                                    break\n",
    "                        \n",
    "                        print(f\"Summary: {summary[:100]}...\")\n",
    "                        \n",
    "                        # Only append if we have at least a headline\n",
    "                        if headline:\n",
    "                            all_articles_data.append({\n",
    "                                'Section': section,\n",
    "                                'Date': date,\n",
    "                                'Headline': headline,\n",
    "                                'Summary': summary\n",
    "                            })\n",
    "                            print(f\"Added article: {headline}\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing article {i} on page {page}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                # Add a small delay between pages to avoid overwhelming the server\n",
    "                if page < end_page:\n",
    "                    time.sleep(2)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing website for page {page}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    finally:\n",
    "        print(\"Closing browser...\")\n",
    "        driver.quit()\n",
    "    \n",
    "    return all_articles_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Saving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_to_csv(articles_data, filename):\n",
    "    if not articles_data:\n",
    "        print(\"No articles to save.\")\n",
    "        return\n",
    "    \n",
    "    fieldnames = ['Section', 'Date', 'Headline', 'Summary']\n",
    "    \n",
    "    with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(articles_data)\n",
    "        print(f\"Saved {len(articles_data)} articles to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Base URL for Northern region\n",
    "base_url = 'https://www.nst.com.my/news/crime-courts'\n",
    "\n",
    "# Set the page range here\n",
    "start_page = 1201  # Change this to your desired starting page\n",
    "end_page = 1300    # Change this to your desired ending page\n",
    "\n",
    "try:\n",
    "    # Scrape the articles from multiple pages\n",
    "    articles_data = scrape_nst_articles(base_url, start_page, end_page)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'nst_articles_{timestamp}.csv'\n",
    "    \n",
    "    # Save to CSV\n",
    "    save_to_csv(articles_data, filename)\n",
    "    \n",
    "    # Print the results\n",
    "    if articles_data:\n",
    "        print(f\"\\nScraped {len(articles_data)} articles from pages {start_page} to {end_page}:\")\n",
    "        for article in articles_data:\n",
    "            print(f\"\\nSection: {article['Section']}\")\n",
    "            print(f\"Date: {article['Date']}\")\n",
    "            print(f\"Headline: {article['Headline']}\")\n",
    "            print(f\"Summary: {article['Summary'][:100]}...\")\n",
    "    else:\n",
    "        print(\"\\nNo articles were found.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "1. Run the installation block first to install required packages\n",
    "2. Run the import block to import necessary libraries\n",
    "3. Run the function definition blocks (WebDriver Setup, Article Scraping, and Data Saving)\n",
    "4. Finally, run the main execution block to start the scraping process\n",
    "\n",
    "The script will:\n",
    "- Scrape articles from the specified page range\n",
    "- Save the results to a CSV file with timestamp\n",
    "- Print progress and results to the console\n",
    "\n",
    "Note: Make sure you have Chrome browser installed on your system before running the script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 