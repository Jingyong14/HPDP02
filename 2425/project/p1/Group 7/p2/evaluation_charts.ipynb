{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_9bkxNR_kTA"
      },
      "source": [
        "# Data Processing\n",
        "This notebook demonstrates unoptimized(pandas) and optimized code for data processing\n",
        "\n",
        "optimized:\n",
        "\n",
        "\n",
        "*   Polars\n",
        "*   Dask\n",
        "* Pyspark\n",
        "* Vectorized pandas\n",
        "\n",
        "## Group Members\n",
        "\n",
        "| No. | Name             |\n",
        "|-----|------------------|\n",
        "| 1   | JOSEPH LAU YEO KAI A22EC0055         |\n",
        "| 2   | VINESH A/L VIJAYA KUMAR A22EC0290   |\n",
        "| 3   | VINESH A/L VIJAYA KUMAR A22EC0290    |\n",
        "| 4   | NUR FARAH ADIBAH BINTI IDRIS A22EC0245       |\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHrhA1Cd_kTB"
      },
      "source": [
        "## 1. Installations and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dateFkxE_kTB"
      },
      "outputs": [],
      "source": [
        "!pip install polars>=0.20.0\n",
        "!pip install \"pymongo[srv]>=4.6.0\"\n",
        "!pip install matplotlib>=3.8.0\n",
        "!pip install seaborn>=0.13.0\n",
        "!pip install psutil>=5.9.0\n",
        "!pip install numpy>=1.24.0\n",
        "\n",
        "\n",
        "import time\n",
        "import psutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import  regex as re\n",
        "from bson import ObjectId\n",
        "from datetime import datetime\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "# polars\n",
        "import polars as pl\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
        "\n",
        "#pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import when, col, regexp_replace, to_date, trim, initcap\n",
        "\n",
        "#dask\n",
        "import dask.dataframe as dd\n",
        "\n",
        "#pandas\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WT65rTo_kTC"
      },
      "source": [
        "## 2. MongoDB Connection and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fYu4QBK_kTC"
      },
      "outputs": [],
      "source": [
        "def connect_mongodb():\n",
        "    \"\"\"Connect to MongoDB and return the client.\"\"\"\n",
        "    uri = \"mongodb+srv://josephyeo:fPya67QIXrl4ZsV5@cluster0.ihjgjas.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "    client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "    try:\n",
        "        client.admin.command('ping')\n",
        "        print(\"Successfully connected to MongoDB!\")\n",
        "        return client\n",
        "    except Exception as e:\n",
        "        print(f\"Error connecting to MongoDB: {e}\")\n",
        "        return None\n",
        "\n",
        "# Connect to MongoDB and load data\n",
        "client = connect_mongodb()\n",
        "db = client[\"mydatabase\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzkvBjmJUPkF"
      },
      "source": [
        "## Load Data from CSV into MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj_xOYFbTS7y"
      },
      "outputs": [],
      "source": [
        "file_path = 'https://raw.githubusercontent.com/Jingyong14/HPDP02/refs/heads/main/2425/project/p1/Group%207/data/raw_data.csv'\n",
        "\n",
        "# Read the CSV\n",
        "df_news = pd.read_csv(file_path)\n",
        "df_news.head()\n",
        "\n",
        "row_count = len(df_news)\n",
        "print(\"Total number of row:\", row_count)\n",
        "\n",
        "# Create or switch to your database\n",
        "db = client[\"mydatabase\"]\n",
        "news_collection = db[\"raw_news\"]\n",
        "\n",
        "# Delete existing data in the collection\n",
        "news_collection.delete_many({})\n",
        "\n",
        "# Insert the all rows into MongoDB\n",
        "news_collection.insert_many(df_news.to_dict(\"records\"))\n",
        "\n",
        "print(\"All rows of news data inserted into MongoDB successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zKbPllk_kTC"
      },
      "source": [
        "## 3. Performance Tracking Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlfzZdYN_kTC"
      },
      "outputs": [],
      "source": [
        "def track_performance(method_name, start_time, start_memory, df):\n",
        "    \"\"\"Track performance metrics for data cleaning operations.\"\"\"\n",
        "    end_time = time.time()\n",
        "    end_memory = psutil.Process().memory_info().rss / (1024 * 1024)  # MB\n",
        "\n",
        "    time_taken = end_time - start_time\n",
        "    throughput = len(df) / time_taken if time_taken > 0 else 0\n",
        "    memory_used = end_memory - start_memory  # in MB\n",
        "\n",
        "    return {\n",
        "        \"Method\": method_name,\n",
        "        \"Time (s)\": time_taken,\n",
        "        \"Throughput (rows/s)\": throughput,\n",
        "        \"Memory Used (MB)\": memory_used\n",
        "    }\n",
        "\n",
        "def track_performance_pyspark(method_name, start_time, start_memory, df=None):\n",
        "    end_time = time.time()\n",
        "    end_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    time_taken = end_time - start_time\n",
        "\n",
        "    if df is not None:\n",
        "        if hasattr(df, \"count\"):  # PySpark DataFrame\n",
        "            row_count = df.count()\n",
        "        else:  # pandas, dask, polars\n",
        "            row_count = len(df)\n",
        "    else:\n",
        "        row_count = 0\n",
        "\n",
        "    throughput = row_count / time_taken if time_taken > 0 else 0\n",
        "    memory_used = end_memory - start_memory  # in MB\n",
        "\n",
        "    return {\n",
        "        \"Method\": method_name,\n",
        "        \"Time (s)\": round(time_taken, 4),\n",
        "        \"Throughput (rows/s)\": round(throughput, 2),\n",
        "        \"Memory Used (MB)\": round(memory_used, 2)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hIcILiVd-Z5"
      },
      "source": [
        "## 4.1 Data Processing with Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uALnTpkHeKk7"
      },
      "outputs": [],
      "source": [
        "# Load documents into a DataFrame\n",
        "db = client[\"mydatabase\"]\n",
        "collection = db['raw_news']\n",
        "data = list(collection.find())\n",
        "df_panda = pd.DataFrame(data)\n",
        "\n",
        "def clean_data(df):\n",
        "    start_time = time.time()\n",
        "    start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    df = df.drop(columns=['_id'])\n",
        "\n",
        "    # Drop nulls and duplicates\n",
        "    df = df.dropna()\n",
        "    df = df.drop_duplicates(keep='first')\n",
        "\n",
        "    # Standardize 'Section' column\n",
        "    if 'Section' in df.columns:\n",
        "        df[\"Section\"] = df[\"Section\"].str.title()\n",
        "\n",
        "    # Clean 'Date' column\n",
        "    def clean_date(date):\n",
        "        if isinstance(date, str):\n",
        "            if \"@\" in date:\n",
        "                date = date.split(\"@\")[0].strip()\n",
        "            return re.sub(r'\\s+', ' ', date)\n",
        "        return date\n",
        "\n",
        "    if 'Date' in df.columns:\n",
        "        df[\"Date\"] = df[\"Date\"].apply(clean_date)\n",
        "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors='coerce')  # Invalid dates become NaT\n",
        "\n",
        "\n",
        "    # Track performance using shared function\n",
        "    performance_report = track_performance(\"Pandas \", start_time, start_memory, df)\n",
        "\n",
        "    return df, performance_report\n",
        "\n",
        "\n",
        "cleaned_df, pandas_result = clean_data(df_panda)\n",
        "\n",
        "print(\"Final row: \", len(cleaned_df))\n",
        "# Save cleaned data to MongoDB\n",
        "cleaned_data = cleaned_df.to_dict(\"records\")\n",
        "db[\"cleaned_data_pandas\"].delete_many({})\n",
        "db[\"cleaned_data_pandas\"].insert_many(cleaned_data)\n",
        "\n",
        "print(\"Cleaned data inserted into 'cleaned_data_pandas' collection in MongoDB\")\n",
        "print(\"\")\n",
        "print(\"Pandas cleaning completed!\")\n",
        "print(pandas_result)\n",
        "print(\"\")\n",
        "\n",
        "# Save cleaned data to CSV\n",
        "cleaned_df.to_csv(\"cleaned_data_unoptimized_pandas.csv\", index=False)\n",
        "print(\"Cleaned data saved to 'cleaned_data_unoptimized_pandas.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MYAdyy4uiOO"
      },
      "source": [
        "## 4.2 Polars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPhvh_T9_kTC"
      },
      "outputs": [],
      "source": [
        "def load_data(client):\n",
        "    \"\"\"Load data from MongoDB into a Polars DataFrame.\"\"\"\n",
        "    db = client[\"mydatabase\"]\n",
        "    collection = db['raw_news']\n",
        "    data = list(collection.find())\n",
        "    df = pl.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "df_polars = load_data(client)\n",
        "\n",
        "def clean_data_polars_default(df):\n",
        "    \"\"\"Clean data using Polars' default processing.\"\"\"\n",
        "    start_time = time.time()\n",
        "    start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    df = df.drop('_id')\n",
        "\n",
        "    # Enable string cache for better performance\n",
        "    with pl.StringCache():\n",
        "\n",
        "        # Define list of fake nulls\n",
        "        fake_nulls = [\"\", \"NaN\", \"null\"]\n",
        "\n",
        "        # Apply replacement for all columns\n",
        "        df = df.with_columns([\n",
        "            pl.when(pl.col(col).is_in(fake_nulls))\n",
        "              .then(None)\n",
        "              .otherwise(pl.col(col))\n",
        "              .alias(col)\n",
        "            for col in df.columns\n",
        "        ])\n",
        "        # Drop duplicates and nulls\n",
        "        df = df.drop_nulls()\n",
        "        df = df.unique()\n",
        "        # Standardize Section\n",
        "        if 'Section' in df.columns:\n",
        "            df = df.with_columns(\n",
        "                df['Section'].str.to_titlecase().alias('Section')\n",
        "            )\n",
        "\n",
        "        # Clean Date\n",
        "        if 'Date' in df.columns:\n",
        "            df = df.with_columns(\n",
        "                df['Date']\n",
        "                .str.split('@')\n",
        "                .list.first()\n",
        "                .str.strip_chars()\n",
        "                .alias('Date')\n",
        "            )\n",
        "\n",
        "            # Convert to datetime\n",
        "            df = df.with_columns(\n",
        "               pl.col('Date').str.strptime(pl.Datetime, format='%b %d, %Y', strict=False).alias('Date')\n",
        "            )\n",
        "            df = df.filter(pl.col(\"Date\").is_not_null())\n",
        "\n",
        "    return df, track_performance(\"Polars\", start_time, start_memory, df)\n",
        "\n",
        "# Run default Polars processing\n",
        "df_polar_cleaned, polar_result = clean_data_polars_default(df_polars)\n",
        "\n",
        "print(\"Final row: \", len(df_polar_cleaned.to_pandas()))\n",
        "# Save cleaned data to MongoDB\n",
        "polar_cleaned = df_polar_cleaned.to_dicts()\n",
        "db[\"cleaned_data_polars\"].delete_many({})\n",
        "db[\"cleaned_data_polars\"].insert_many(polar_cleaned)\n",
        "print(\"Cleaned data inserted into 'cleaned_data_polars' collection in MongoDB\")\n",
        "print(\"\")\n",
        "\n",
        "df_polar_cleaned = df_polar_cleaned.to_pandas().to_csv(\"cleaned_data_optimized_polar.csv\", index=False)\n",
        "print(\"Cleaned data saved to 'cleaned_data_optimized_polar.csv'\")\n",
        "print(\"\")\n",
        "\n",
        "print(\"Polars cleaning completed!\")\n",
        "print(polar_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK6XczVBu-nT"
      },
      "source": [
        "## 4.3 Dask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0zftw4Gv8OM"
      },
      "outputs": [],
      "source": [
        "df_dask = dd.from_pandas(df_panda, npartitions=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CDo9272xFln"
      },
      "outputs": [],
      "source": [
        "# Function to clean data using Dask (default scheduler)\n",
        "def clean_data_dask_default(df_dask):\n",
        "    start_time = time.time()\n",
        "    start_memory = psutil.Process().memory_info().rss / (1024 * 1024)  # MB\n",
        "\n",
        "    df_cleaned = df_dask.drop(columns=['_id'])\n",
        "\n",
        "    df_cleaned = df_cleaned.map_partitions(lambda df: df.dropna())\n",
        "\n",
        "    # Perform drop_duplicates() in Pandas (to check across all rows), then switch back to Dask\n",
        "    df_cleaned_pandas = df_cleaned.compute()\n",
        "    df_cleaned_pandas = df_cleaned_pandas.drop_duplicates(subset=[\"Section\", \"Date\", \"Headline\", \"Summary\"])\n",
        "\n",
        "    # Convert back to Dask DataFrame after dropping duplicates\n",
        "    df_cleaned = dd.from_pandas(df_cleaned_pandas, npartitions=4)\n",
        "\n",
        "    df_cleaned = df_cleaned.map_partitions(lambda df: df.assign(Section=df['Section'].str.title()))\n",
        "    df_cleaned = df_cleaned.map_partitions(lambda df: df.assign(Date=df['Date'].map(lambda x: x.split('@')[0].strip() if isinstance(x, str) else x)))\n",
        "    df_cleaned = df_cleaned.map_partitions(lambda df: df.assign(Date=pd.to_datetime(df['Date'], errors='coerce')))\n",
        "\n",
        "    # Compute to Pandas\n",
        "    cleaned_df = df_cleaned.compute()\n",
        "\n",
        "    # Track performance\n",
        "    performance_report = track_performance(\"Dask \", start_time, start_memory, cleaned_df)\n",
        "\n",
        "    return cleaned_df, performance_report\n",
        "\n",
        "df_dask_cleaned, dask_result = clean_data_dask_default(df_dask)\n",
        "\n",
        "print(\"Final row: \", len(df_dask_cleaned))\n",
        "# Save cleaned data to MongoDB\n",
        "dask_cleaned = df_dask_cleaned.to_dict(\"records\")\n",
        "db[\"cleaned_data_dask\"].delete_many({})\n",
        "db[\"cleaned_data_dask\"].insert_many(dask_cleaned)\n",
        "\n",
        "\n",
        "df_dask_cleaned.to_csv(\"cleaned_data_optimized_dask.csv\", index=False)\n",
        "print(\"Cleaned data saved to 'cleaned_data_optimized_dask.csv'\")\n",
        "print(\"\")\n",
        "\n",
        "print(\"Cleaned data inserted into 'cleaned_data_dask' collection in MongoDB\")\n",
        "print(\"\")\n",
        "print(\"Dask cleaning completed!\")\n",
        "print(dask_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QoP9fYbj3Wc"
      },
      "source": [
        "## 4.4 PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SencrQCAHd5h"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_data_pyspark(df_spark):\n",
        "    start_time = time.time()\n",
        "    start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    # Drop NA and Duplicates\n",
        "    df_cleaned= df_spark.drop('_id')\n",
        "\n",
        "    # List of \"fake\" nulls to replace\n",
        "    fake_nulls = [\"\", \"NaN\", \"null\"]\n",
        "\n",
        "    # Replace in all columns\n",
        "    for c in df_cleaned.columns:\n",
        "        df_cleaned = df_cleaned.withColumn(c,\n",
        "            when(col(c).isin(fake_nulls), None).otherwise(col(c))\n",
        "        )\n",
        "\n",
        "    df_cleaned = df_cleaned.dropna()\n",
        "    df_cleaned = df_cleaned.dropDuplicates()\n",
        "\n",
        "    # Clean and convert the Date column\n",
        "    df_cleaned = df_cleaned.withColumn(\"Date\", regexp_replace(col(\"Date\"), \"@.*$\", \"\"))  # Remove time part\n",
        "    df_cleaned = df_cleaned.withColumn(\"Date\", regexp_replace(col(\"Date\"), \"\\s+\", \" \"))  # Normalize spaces\n",
        "    df_cleaned = df_cleaned.withColumn(\"Date\", trim(col(\"Date\")))  # Trim spaces\n",
        "    df_cleaned = df_cleaned.withColumn(\"Date\", to_date(col(\"Date\"), \"MMM d, yyyy\"))  # Parse to date\n",
        "    df_cleaned = df_cleaned.filter(col(\"Date\").isNotNull())\n",
        "\n",
        "\n",
        "    # Format Section to Title Case\n",
        "    df_cleaned = df_cleaned.withColumn(\"Section\", initcap(col(\"Section\")))\n",
        "\n",
        "    return df_cleaned, track_performance_pyspark(\"PySpark\", start_time, start_memory, df_cleaned)\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.appName(\"CleanNewsData\").getOrCreate()\n",
        "\n",
        "# Convert MongoDB ObjectId to string\n",
        "df_panda[\"_id\"] = df_panda[\"_id\"].astype(str)\n",
        "\n",
        "# Convert to PySpark DataFrame\n",
        "df_spark = spark.createDataFrame(df_panda)\n",
        "\n",
        "# Clean and track performance\n",
        "df_spark_cleaned, pyspark_result = clean_data_pyspark(df_spark)\n",
        "\n",
        "# Save cleaned data to MongoDB\n",
        "# Convert PySpark DataFrame to Pandas DataFrame\n",
        "df_spark_cleaned_pandas = df_spark_cleaned.toPandas()\n",
        "\n",
        "# Fix dates in pandas df\n",
        "df_spark_cleaned_pandas = df_spark_cleaned_pandas[pd.to_datetime(df_spark_cleaned_pandas[\"Date\"], errors=\"coerce\").notna()]\n",
        "df_spark_cleaned_pandas[\"Date\"] = pd.to_datetime(df_spark_cleaned_pandas[\"Date\"])\n",
        "print(\"Final row: \", len(df_spark_cleaned_pandas))\n",
        "\n",
        "# Convert to dictionary for MongoDB insertion\n",
        "spark_cleaned = df_spark_cleaned_pandas.to_dict(\"records\")\n",
        "db[\"cleaned_data_pyspark\"].delete_many({})\n",
        "db[\"cleaned_data_pyspark\"].insert_many(spark_cleaned)\n",
        "print(\"Cleaned data inserted into 'cleaned_data_pyspark' collection in MongoDB\\n\")\n",
        "\n",
        "# Save to CSV\n",
        "df_spark_cleaned_pandas.to_csv(\"cleaned_data_optimized_pyspark.csv\", index=False)\n",
        "print(\"Cleaned data saved to 'cleaned_data_optimized_pyspark.csv'\\n\")\n",
        "\n",
        "print(\"Pyspark cleaning completed!\")\n",
        "print(pyspark_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8goyHETakDoL"
      },
      "source": [
        "## 4.5 Vectorized Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vq5_V02hmwHP"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Optimized vectorized cleaning function\n",
        "def clean_data_pandas_vectorized(df):\n",
        "    start_time = time.time()\n",
        "    start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    # Drop nulls and duplicates in one go (vectorized)\n",
        "    df = df.drop(columns=['_id'])\n",
        "    df = df.dropna().drop_duplicates()\n",
        "\n",
        "    # Standardize 'Section' column (vectorized str methods)\n",
        "    if 'Section' in df.columns:\n",
        "        df['Section'] = df['Section'].str.title()\n",
        "\n",
        "    # Clean 'Date' column (vectorized string methods + datetime)\n",
        "    if 'Date' in df.columns:\n",
        "        # Clean strings using vectorized apply (no need for a loop)\n",
        "        df['Date'] = df['Date'].str.split('@').str[0].str.strip()\n",
        "        df['Date'] = df['Date'].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "        # Convert to datetime in one call (vectorized)\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "    # Track performance (reuse track_performance function)\n",
        "    performance_report = track_performance(\"Vectorized Pandas\", start_time, start_memory, df)\n",
        "\n",
        "    return df, performance_report\n",
        "\n",
        "# Test the function\n",
        "df_vectorized_cleaned, vectorized_result = clean_data_pandas_vectorized(df_panda)\n",
        "\n",
        "# Save cleaned data to MongoDB\n",
        "vectorized_cleaned = df_vectorized_cleaned.to_dict(\"records\")\n",
        "db[\"cleaned_data_vectorized_pandas\"].delete_many({})\n",
        "db[\"cleaned_data_vectorized_pandas\"].insert_many(vectorized_cleaned)\n",
        "print(\"Cleaned data inserted into 'cleaned_data_vectorized_pandas' collection in MongoDB\")\n",
        "print(\"\")\n",
        "\n",
        "df_vectorized_cleaned.to_csv(\"cleaned_data_optimized_vectorized.csv\", index=False)\n",
        "print(\"Cleaned data saved to 'cleaned_data_optimized_vectorized.csv'\")\n",
        "print(\"\")\n",
        "print(\"Vectorized pandas cleaning completed!\")\n",
        "print(vectorized_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1RDQKQkfOYt"
      },
      "source": [
        "## 5. Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ZTIEyk_kTD"
      },
      "source": [
        "### 5.1 Plot Results for Pandas vs Polars\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN3kOZoJ_kTE"
      },
      "outputs": [],
      "source": [
        "def plot_results(results):\n",
        "    \"\"\"Plot performance comparison results in 3 columns\"\"\"\n",
        "    # Convert results to pandas DataFrame for plotting\n",
        "    results_df = pl.DataFrame(results).to_pandas()\n",
        "\n",
        "    # Set plot style\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Define metrics and color palette\n",
        "    metrics = [\"Time (s)\", \"Throughput (rows/s)\", \"Memory Used (MB)\"]\n",
        "    palette = sns.color_palette(\"pastel\", n_colors=len(results_df['Method'].unique()))\n",
        "\n",
        "    # Create a single row of 3 subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axes[i]\n",
        "        sns.barplot(\n",
        "            x=\"Method\", y=metric, hue=\"Method\", legend=False,\n",
        "            data=results_df, palette=palette, ax=ax\n",
        "        )\n",
        "\n",
        "        # Add values on top of the bars\n",
        "        for p in ax.patches:\n",
        "            ax.annotate(f'{p.get_height():.2f}',\n",
        "                        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                        ha='center', va='center', fontsize=10, color='black',\n",
        "                        xytext=(0, 5), textcoords='offset points')\n",
        "\n",
        "        ax.set_title(f\"{metric} Comparison\", fontsize=14)\n",
        "        ax.set_xlabel(\"\")\n",
        "        ax.set_ylabel(metric)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Collect all results\n",
        "results = [polar_result, pandas_result]\n",
        "\n",
        "# Plot results\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCnBR_8MxSCX"
      },
      "source": [
        "### 5.2 Plot Results for Pandas vs Dask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8JbhCMly3al"
      },
      "outputs": [],
      "source": [
        "# Collect all results\n",
        "results_dask = [dask_result, pandas_result]\n",
        "\n",
        "# Plot results\n",
        "plot_results(results_dask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF9fbJb-kzUd"
      },
      "source": [
        "### 5.3 Plot Results for Pandas vs PySpark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHflPIk8pm4M"
      },
      "outputs": [],
      "source": [
        "# Collect all results\n",
        "results_pyspark = [pyspark_result, pandas_result]\n",
        "\n",
        "# Plot results\n",
        "plot_results(results_pyspark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOtO57FHkzmu"
      },
      "source": [
        "### 5.4 Plot Results for Pandas vs Vectorized Pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp1t19ePnnlK"
      },
      "outputs": [],
      "source": [
        "# Collect all results\n",
        "results_pandas = [vectorized_result, pandas_result]\n",
        "\n",
        "# Plot results\n",
        "plot_results(results_pandas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbP40lW1Yl82"
      },
      "source": [
        "### 5.5 Plot Results for all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDDuDHPhYolT"
      },
      "outputs": [],
      "source": [
        "def plot_results_overall(results):\n",
        "    \"\"\"Plot performance comparison results in 3 columns (side-by-side).\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import polars as pl\n",
        "\n",
        "    # Convert results to pandas DataFrame\n",
        "    results_df = pl.DataFrame(results).to_pandas()\n",
        "\n",
        "    # Set seaborn style\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Define metrics and color palette (5 distinct pastel colors)\n",
        "    metrics = [\"Time (s)\", \"Throughput (rows/s)\", \"Memory Used (MB)\"]\n",
        "    unique_methods = results_df['Method'].unique()\n",
        "    palette = dict(zip(unique_methods, sns.color_palette(\"pastel\", n_colors=len(unique_methods))))\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axes[i]\n",
        "        sns.barplot(\n",
        "            x=\"Method\", y=metric, hue=\"Method\",\n",
        "            data=results_df, palette=palette, legend=False, ax=ax\n",
        "        )\n",
        "\n",
        "        # Annotate bar values\n",
        "        for p in ax.patches:\n",
        "            height = p.get_height()\n",
        "            ax.annotate(f'{height:.2f}',\n",
        "                        (p.get_x() + p.get_width() / 2., height),\n",
        "                        ha='center', va='bottom', fontsize=9, color='black',\n",
        "                        xytext=(0, 5), textcoords='offset points')\n",
        "\n",
        "        ax.set_title(f\"{metric} Comparison\", fontsize=13)\n",
        "        ax.set_xlabel(\"\")\n",
        "        ax.set_ylabel(metric)\n",
        "        ax.tick_params(axis='x', labelrotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Collect all results\n",
        "overall = [pandas_result, vectorized_result, polar_result, dask_result, pyspark_result ]\n",
        "\n",
        "# Plot results\n",
        "plot_results_overall(overall)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export data to csv\n",
        "from google.colab import files  # Add this import for Colab\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "results_df = pd.DataFrame(overall)\n",
        "\n",
        "# Format the numeric columns (optional)\n",
        "results_df['Time (s)'] = results_df['Time (s)'].round(4)\n",
        "results_df['Throughput (rows/s)'] = results_df['Throughput (rows/s)'].round(4)\n",
        "results_df['Memory Used (MB)'] = results_df['Memory Used (MB)'].round(4)\n",
        "\n",
        "# Save to CSV\n",
        "results_df.to_csv('performance_results.csv', index=False)\n",
        "\n",
        "# Download the file\n",
        "files.download('performance_results.csv')\n",
        "\n",
        "# Print confirmation\n",
        "print(\"Results exported and downloaded as 'performance_results.csv'\")\n",
        "print(\"\\nResults summary:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "l1DXlB3tnxRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnLPFg0N_kTE"
      },
      "source": [
        "## 6. Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bogcaqtG_kTE"
      },
      "outputs": [],
      "source": [
        "# Close the MongoDB connection\n",
        "client.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF1X4cfAOahL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}