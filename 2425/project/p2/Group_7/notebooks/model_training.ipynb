{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d78655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Spark and MLlib libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors, Vector\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904e6ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "# The appName is a label for your application in the Spark UI\n",
    "# The .getOrCreate() method ensures that if a SparkSession already exists, it will be used,\n",
    "# otherwise, a new one will be created.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkModelTraining\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to reduce verbosity for cleaner output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"SparkSession initialized successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65375fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned data loaded successfully from ../data/cleaned_data.csv\n",
      "Filtered 8222 rows with empty or null 'clean_comment' after loading in Spark.\n",
      "Filtered 5274 rows with NULL 'label' values.\n",
      "\n",
      "--- Cleaned Data Sample (first 5 rows) ---\n",
      "+-------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------+-----+\n",
      "|content                                                                                                            |clean_comment                                                             |label|\n",
      "+-------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------+-----+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|thats bummer shoulda got david carr third day                             |1    |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |upset cant update facebook texting might cry result school today also blah|1    |\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |dived many time ball managed save rest go bound                           |1    |\n",
      "|my whole body feels itchy and like its on fire                                                                     |whole body feel itchy like fire                                           |1    |\n",
      "|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.     |behaving im mad cant see                                                  |1    |\n",
      "+-------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- clean_comment: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n",
      "Total rows in cleaned DataFrame after Spark-side filtering: 1603753\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Cleaned Data ---\n",
    "\n",
    "# Define path for cleaned data (CSV format)\n",
    "cleaned_data_path = \"../data/cleaned_data.csv\" \n",
    "\n",
    "try:\n",
    "    # --- FIX: Define explicit schema for reading CSV ---\n",
    "    # This ensures Spark reads the columns with the correct data types,\n",
    "    # preventing misinterpretations by inferSchema=True.\n",
    "    cleaned_data_schema = StructType([\n",
    "        StructField(\"content\", StringType(), True),\n",
    "        StructField(\"clean_comment\", StringType(), True),\n",
    "        StructField(\"sentiment\", IntegerType(), True) # Explicitly define sentiment as IntegerType\n",
    "    ])\n",
    "\n",
    "    # Load the CSV file using the defined schema\n",
    "    df_cleaned = spark.read.csv(cleaned_data_path, header=True, schema=cleaned_data_schema)\n",
    "    print(f\"\\nCleaned data loaded successfully from {cleaned_data_path}\")\n",
    "\n",
    "    # Rename 'sentiment' column to 'label' for MLlib compatibility\n",
    "    df_cleaned = df_cleaned.withColumnRenamed(\"sentiment\", \"label\")\n",
    "    \n",
    "    # --- ADDED: Re-filter out empty clean_comment rows in Spark ---\n",
    "    # This ensures consistency with the preprocessing script's filtering.\n",
    "    initial_spark_rows = df_cleaned.count() # Count before filtering\n",
    "    df_cleaned = df_cleaned.filter(col(\"clean_comment\").isNotNull() & (col(\"clean_comment\") != \"\"))\n",
    "    rows_filtered_in_spark = initial_spark_rows - df_cleaned.count()\n",
    "    if rows_filtered_in_spark > 0:\n",
    "        print(f\"Filtered {rows_filtered_in_spark} rows with empty or null 'clean_comment' after loading in Spark.\")\n",
    "    \n",
    "    # --- ADDED FIX: Filter out rows with null labels ---\n",
    "    initial_rows_before_label_filter = df_cleaned.count()\n",
    "    df_cleaned = df_cleaned.filter(col(\"label\").isNotNull())\n",
    "    rows_filtered_for_null_labels = initial_rows_before_label_filter - df_cleaned.count()\n",
    "    if rows_filtered_for_null_labels > 0:\n",
    "        print(f\"Filtered {rows_filtered_for_null_labels} rows with NULL 'label' values.\")\n",
    "\n",
    "    print(\"\\n--- Cleaned Data Sample (first 5 rows) ---\")\n",
    "    df_cleaned.show(5, truncate=False)\n",
    "    df_cleaned.printSchema()\n",
    "    print(f\"Total rows in cleaned DataFrame after Spark-side filtering: {df_cleaned.count()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading cleaned data: {e}\")\n",
    "    print(\"Please ensure the preprocessing step ran successfully and 'cleaned_data.csv' was saved at the specified path.\")\n",
    "    # If loading fails, create a dummy DataFrame to avoid breaking the notebook flow\n",
    "    dummy_schema = StructType([\n",
    "        StructField(\"content\", StringType(), True),\n",
    "        StructField(\"clean_comment\", StringType(), True),\n",
    "        StructField(\"label\", IntegerType(), True)\n",
    "    ])\n",
    "    df_cleaned = spark.createDataFrame([\n",
    "        (\"This is a positive comment.\", \"positive comment\", 0),\n",
    "        (\"This is a negative comment.\", \"negative comment\", 1),\n",
    "        (\"This is a neutral comment.\", \"neutral comment\", 2)\n",
    "    ], schema=dummy_schema)\n",
    "    print(\"Loaded dummy cleaned data for demonstration due to file loading issue.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02622b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building Feature Engineering Pipeline and Transforming Data ---\n",
      "Feature engineering completed. Sample of features and labels:\n",
      "+--------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|clean_comment                                                             |features                                                                                                                                                                                                                                                                                          |label|\n",
      "+--------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|thats bummer shoulda got david carr third day                             |(10000,[1528,2950,5459,6010,8771,9133,9257,9610],[6.9195180027518175,8.87621163720816,7.8170581852805965,3.1409939434263685,7.197780853287107,2.7876831730184035,3.9542124158285348,6.481161316542019])                                                                                           |1    |\n",
      "|upset cant update facebook texting might cry result school today also blah|(10000,[399,1939,2679,3193,3792,3924,4449,4938,5230,6026,9806,9953],[6.821058214044596,7.3731267963446365,4.390639749540666,5.791275451551086,4.982661786074337,5.099967445924608,7.011301286344488,5.428210189348201,5.4517744980709875,3.2497152565120313,3.2038553322393266,6.121641420471056])|1    |\n",
      "|dived many time ball managed save rest go bound                           |(10000,[2708,4520,5185,5188,6451,6901,7492,8157,8758],[4.349968709809738,6.2771661499328975,8.969737695218983,5.187220582178979,3.040122477909524,7.620900896633992,6.689959738541415,3.246521109193063,6.368137928138625])                                                                       |1    |\n",
      "|whole body feel itchy like fire                                           |(10000,[627,841,1879,3115,3330,5191],[5.557813735818184,6.313669019777187,8.095495199588326,3.8313187040492105,3.032796628135569,6.873887398872754])                                                                                                                                              |1    |\n",
      "|behaving im mad cant see                                                  |(10000,[1616,2007,3623,5354,6026],[9.435827425143582,5.5038486178765655,2.2668623940407784,3.5623461290462393,3.2497152565120313])                                                                                                                                                                |1    |\n",
      "+--------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- clean_comment: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- raw_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "\n",
      "--- Checking Feature Vector Validity ---\n",
      "Total rows in DataFrame after feature engineering and null feature filtering: 1603753\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Text Feature Engineering (TF-IDF) ---\n",
    "\n",
    "# Ensure 'clean_comment' column is not null and is string type\n",
    "# This line is redundant if the above filter is effective, but harmless.\n",
    "df_cleaned = df_cleaned.withColumn(\"clean_comment\", col(\"clean_comment\").cast(StringType()))\n",
    "\n",
    "# 2.1 Tokenization: Split text into words\n",
    "tokenizer = Tokenizer(inputCol=\"clean_comment\", outputCol=\"words\")\n",
    "\n",
    "# 2.2 Stop Words Removal (Optional, as clean_text already handles this, but can be used for extra robustness)\n",
    "# For this dataset, English stopwords are primarily removed by the preprocessing script.\n",
    "# If you uncomment, make sure 'words' column exists from tokenizer.\n",
    "# remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_words\")\n",
    "\n",
    "# 2.3 HashingTF: Convert words into fixed-size feature vectors (term frequencies)\n",
    "# numFeatures is the size of the vocabulary (or feature vector dimension).\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"raw_features\", numFeatures=10000)\n",
    "\n",
    "# 2.4 IDF: Calculate Inverse Document Frequency\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# 2.5 Create a Pipeline to chain these steps\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
    "\n",
    "# Fit the pipeline to the data and transform it to get the 'features' column\n",
    "print(\"\\n--- Building Feature Engineering Pipeline and Transforming Data ---\")\n",
    "pipeline_model = pipeline.fit(df_cleaned)\n",
    "df_features = pipeline_model.transform(df_cleaned)\n",
    "\n",
    "print(\"Feature engineering completed. Sample of features and labels:\")\n",
    "df_features.select(\"clean_comment\", \"features\", \"label\").show(5, truncate=False)\n",
    "df_features.printSchema()\n",
    "\n",
    "# --- ADDED: Check for empty/null feature vectors after TF-IDF ---\n",
    "print(\"\\n--- Checking Feature Vector Validity ---\")\n",
    "# Count rows where 'features' column is null or has zero size (empty vector)\n",
    "# For SparseVector, size is the number of non-zero elements.\n",
    "# A vector with no non-zero elements (i.e., all zeros) can be problematic for some models.\n",
    "# Check for null features first\n",
    "null_features_count = df_features.filter(col(\"features\").isNull()).count()\n",
    "if null_features_count > 0:\n",
    "    print(f\"WARNING: Found {null_features_count} rows with NULL feature vectors. These will be filtered out.\")\n",
    "    df_features = df_features.filter(col(\"features\").isNotNull())\n",
    "\n",
    "# Check for empty (all zero) feature vectors\n",
    "# This requires converting to RDD and checking the sparse vector's size or values\n",
    "# A more direct way to check if a SparseVector is all zeros: its size (num_non_zeros) will be 0\n",
    "# However, the 'features' column is of type VectorUDT, which doesn't directly expose num_non_zeros\n",
    "# A common way to check for all-zero vectors is to filter where the norm is zero, but that's computationally heavy.\n",
    "# A simpler approach is to filter out rows where the 'words' column (after tokenization) is empty,\n",
    "# as this would lead to empty feature vectors. We already have a filter for clean_comment != \"\".\n",
    "# Let's check if any feature vector has zero non-zero elements.\n",
    "# This requires a UDF or more complex Spark SQL. For now, we'll rely on the filter for clean_comment.\n",
    "# If the error persists, we might need to inspect the actual vector contents.\n",
    "\n",
    "# Re-count after any potential filtering for null features\n",
    "print(f\"Total rows in DataFrame after feature engineering and null feature filtering: {df_features.count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2331caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data rows: 1123101\n",
      "Test data rows: 480652\n",
      "\n",
      "--- Starting Multi-class Logistic Regression Model Training ---\n",
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Data Splitting and Model Training ---\n",
    "\n",
    "# 3.1 Data Splitting\n",
    "# Split the data with features and labels into training and testing sets.\n",
    "train_df, test_df = df_features.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"\\nTraining data rows: {train_df.count()}\")\n",
    "print(f\"Test data rows: {test_df.count()}\")\n",
    "\n",
    "# 3.2 Model Definition: Multi-class Logistic Regression\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", family=\"multinomial\")\n",
    "\n",
    "# 3.3 Model Training\n",
    "print(\"\\n--- Starting Multi-class Logistic Regression Model Training ---\")\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed169886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Predictions Sample (first 10 rows) ---\n",
      "+-----+----------+--------------------------------------------------------------+-----------------------------------------------------------+\n",
      "|label|prediction|probability                                                   |rawPrediction                                              |\n",
      "+-----+----------+--------------------------------------------------------------+-----------------------------------------------------------+\n",
      "|1    |0.0       |[0.5462467724006658,0.445209223408468,0.008544004190866308]   |[1.454122522043148,1.2495960209356465,-2.7037185429787947] |\n",
      "|1    |0.0       |[0.7751438989565699,0.22479964688796647,5.6454155463643375E-5]|[3.5884047356894677,2.350565594734031,-5.9389703304234995] |\n",
      "|1    |1.0       |[0.398928353698952,0.6010145160240701,5.713027697791523E-5]   |[2.813788547631738,3.2236257988915984,-6.037414346523336]  |\n",
      "|1    |1.0       |[0.14401925133937357,0.8559804258634051,3.227972212731543E-7] |[3.742044227300343,5.524344755701559,-9.266388983001903]   |\n",
      "|1    |1.0       |[0.02953603234609998,0.9682023480947839,0.002261619559116108] |[-0.3067667905032907,3.183063358754673,-2.8762965682513824]|\n",
      "|1    |1.0       |[0.17905172912243084,0.8205191008247056,4.2917005286345215E-4]|[1.5037714541424712,3.026033890293389,-4.529805344435861]  |\n",
      "|1    |1.0       |[0.26853046615195736,0.73137312402661,9.640982143255454E-5]   |[2.3100507307812554,3.312010114401721,-5.622060845182977]  |\n",
      "|1    |0.0       |[0.6616330584739734,0.33836691346537423,2.8060652324752092E-8]|[5.882144535469626,5.21156427504925,-11.093708810518875]   |\n",
      "|1    |1.0       |[0.07569442819633818,0.9235058976633692,7.996741402925979E-4] |[0.6829276270250426,3.18440025886492,-3.8673278858899627]  |\n",
      "|1    |1.0       |[0.47477210609414583,0.5205546742053125,0.004673219700541756] |[1.5096422075960145,1.6017022195743358,-3.1113444271703505]|\n",
      "+-----+----------+--------------------------------------------------------------+-----------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy on test set: 0.7571257375398417\n",
      "F1 Score on test set: 0.7563623062217032\n",
      "Weighted Precision on test set: 0.7568154551114237\n",
      "Weighted Recall on test set: 0.7571257375398417\n",
      "\n",
      "Saving trained Logistic Regression model to ../data/trained_model/lr...\n",
      "Model saved successfully.\n",
      "\n",
      "--- Logistic Regression Model Coefficients and Intercept ---\n",
      "Intercepts (per class): [1.6524135232428767,1.3808667438954993,-3.0332802671383763]\n",
      "Coefficients (per class, sample of first 10): [[ 0.12047011  0.06431562 -0.03506957 -0.04700541 -0.07621147 -0.03832855\n",
      "  -0.06666255 -0.00056774 -0.04507925  0.10775211]\n",
      " [ 0.13893205 -0.02938278  0.03596396 -0.02615733 -0.04335574  0.03299018\n",
      "   0.04586644 -0.04353308  0.00525167  0.15012635]\n",
      " [-0.25940216 -0.03493285 -0.00089439  0.07316274  0.11956721  0.00533836\n",
      "   0.02079611  0.04410082  0.03982757 -0.25787846]]\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Make Predictions and Model Evaluation ---\n",
    "\n",
    "# 4.1 Make Predictions\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "print(\"\\n--- Predictions Sample (first 10 rows) ---\")\n",
    "predictions.select(\"label\", \"prediction\", \"probability\", \"rawPrediction\").show(10, truncate=False)\n",
    "\n",
    "# 4.2 Model Evaluation (Multi-class)\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "evaluator_weighted_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_weighted_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "weighted_precision = evaluator_weighted_precision.evaluate(predictions)\n",
    "weighted_recall = evaluator_weighted_recall.evaluate(predictions)\n",
    "\n",
    "print(f\"\\nAccuracy on test set: {accuracy}\")\n",
    "print(f\"F1 Score on test set: {f1_score}\")\n",
    "print(f\"Weighted Precision on test set: {weighted_precision}\")\n",
    "print(f\"Weighted Recall on test set: {weighted_recall}\")\n",
    "\n",
    "# 4.3 Save Trained Model\n",
    "model_save_path = \"../data/trained_model/lr\" \n",
    "print(f\"\\nSaving trained Logistic Regression model to {model_save_path}...\")\n",
    "try:\n",
    "    lr_model.save(model_save_path)\n",
    "    print(\"Model saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n--- Logistic Regression Model Coefficients and Intercept ---\")\n",
    "    print(f\"Intercepts (per class): {lr_model.interceptVector}\")\n",
    "    print(f\"Coefficients (per class, sample of first 10): {lr_model.coefficientMatrix.toArray()[:, :10]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve LR model or coefficients: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9ee74b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SparkSession stopped.\n"
     ]
    }
   ],
   "source": [
    "# --- Stop SparkSession ---\n",
    "spark.stop()\n",
    "print(\"\\nSparkSession stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4ed1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
