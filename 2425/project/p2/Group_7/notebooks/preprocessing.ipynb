{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "451d032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HAFIZI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HAFIZI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HAFIZI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b2b86a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK 'wordnet' corpus not found. Downloading...\n",
      "NLTK 'wordnet' corpus downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HAFIZI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"NLTK stopwords not found. Downloading...\")\n",
    "    nltk.download('stopwords')\n",
    "    print(\"NLTK stopwords downloaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while checking/downloading NLTK stopwords: {e}\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"NLTK 'punkt' tokenizer not found. Downloading...\")\n",
    "    nltk.download('punkt')\n",
    "    print(\"NLTK 'punkt' tokenizer downloaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while checking/downloading NLTK 'punkt' tokenizer: {e}\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    print(\"NLTK 'wordnet' corpus not found. Downloading...\")\n",
    "    nltk.download('wordnet')\n",
    "    print(\"NLTK 'wordnet' corpus downloaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while checking/downloading NLTK 'wordnet' corpus: {e}\")\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# --- Text Cleaning Function (Updated with Tokenization and Lemmatization) ---\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # 3. Remove mentions (@usernames)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 4. Remove hashtag symbols but keep the word (e.g., #awesome -> awesome)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # 5. Remove special characters and numbers, keeping only letters and spaces\n",
    "    # This also effectively handles punctuation removal\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 6. Consolidate multiple spaces to single spaces and trim leading/trailing spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 7. Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # 8. Remove English stop words and Lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a5244d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Sentiment Labeling Function for Reddit Data (keyword-based) ---\n",
    "positive_words = ['good', 'great', 'excellent', 'happy', 'love', 'best', 'amazing', 'nice', 'wonderful', 'awesome']\n",
    "negative_words = ['bad', 'terrible', 'worst', 'sad', 'hate', 'awful', 'horrible', 'poor', 'angry', 'disappointed']\n",
    "\n",
    "def label_sentiment_keyword_based(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 2 # Neutral or undefined for non-string input\n",
    "\n",
    "    positive_count = sum(1 for word in positive_words if word in text)\n",
    "    negative_count = sum(1 for word in negative_words if word in text)\n",
    "\n",
    "    if positive_count > negative_count:\n",
    "        return 0  # Positive\n",
    "    elif negative_count > positive_count:\n",
    "        return 1  # Negative\n",
    "    else:\n",
    "        return 2  # Neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffebc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e9d6665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected raw data directory: c:\\Users\\HAFIZI\\Documents\\HPDP\\reddit_sentiment_workflow\\data\\raw_data\n",
      "Expected output directory: c:\\Users\\HAFIZI\\Documents\\HPDP\\reddit_sentiment_workflow\\data\n",
      "--- Loading and Initial Processing of Sentiment140 Data ---\n",
      "Sentiment140 data loaded successfully from c:\\Users\\HAFIZI\\Documents\\HPDP\\reddit_sentiment_workflow\\data\\raw_data\\Sentiment140.csv\n",
      "Sentiment140 data initial rows: 1600000\n",
      "\n",
      "--- Loading and Initial Processing of Reddit Data ---\n",
      "Reddit data initial rows: 13614\n",
      "\n",
      "--- Combining DataFrames ---\n",
      "Total rows in combined DataFrame before cleaning: 1613614\n",
      "\n",
      "--- Applying Text Cleaning to Combined Data ---\n",
      "Rows after text cleaning: 1605730 (dropped 7884 empty clean comments)\n",
      "\n",
      "--- Applying Sentiment Labeling for Reddit Data based on cleaned text ---\n",
      "Sentiment applied to 13444 Reddit rows.\n",
      "\n",
      "Final combined cleaned and labeled data saved to c:\\Users\\HAFIZI\\Documents\\HPDP\\reddit_sentiment_workflow\\data\\cleaned_data.csv\n",
      "Final number of rows in cleaned_data.csv: 1605730\n",
      "\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Processing Logic ---\n",
    "def process_data_and_combine(sentiment140_file_path, reddit_jsonl_files):\n",
    "    # --- 1. Process Sentiment140 Data ---\n",
    "    print(\"--- Loading and Initial Processing of Sentiment140 Data ---\")\n",
    "    sentiment140_column_names = [\"polarity_raw\", \"tweet_id\", \"date\", \"query\", \"user\", \"tweet_text\"]\n",
    "    try:\n",
    "        df_s140 = pd.read_csv(sentiment140_file_path, header=None, names=sentiment140_column_names, encoding='ISO-8859-1')\n",
    "        print(f\"Sentiment140 data loaded successfully from {sentiment140_file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Sentiment140.csv not found at {sentiment140_file_path}. Please ensure the path is correct.\")\n",
    "        df_s140 = pd.DataFrame(columns=['content', 'sentiment', 'source'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Sentiment140 data: {e}\")\n",
    "        df_s140 = pd.DataFrame(columns=['content', 'sentiment', 'source'])\n",
    "\n",
    "    if not df_s140.empty:\n",
    "        df_s140 = df_s140.rename(columns={\"tweet_text\": \"content\"})\n",
    "        # Map polarity_raw: 0 (negative) -> 1, 2 (neutral) -> 2, 4 (positive) -> 0\n",
    "        df_s140['sentiment'] = df_s140['polarity_raw'].map({0: 1, 2: 2, 4: 0})\n",
    "        df_s140 = df_s140.dropna(subset=['sentiment']).copy()\n",
    "        df_s140['sentiment'] = df_s140['sentiment'].astype(int)\n",
    "        df_s140['source'] = 'sentiment140'\n",
    "        df_s140 = df_s140[['content', 'sentiment', 'source']]\n",
    "        print(f\"Sentiment140 data initial rows: {len(df_s140)}\")\n",
    "    else:\n",
    "        print(\"Sentiment140 DataFrame is empty or failed to load.\")\n",
    "\n",
    "    # --- 2. Process Reddit Data ---\n",
    "    print(\"\\n--- Loading and Initial Processing of Reddit Data ---\")\n",
    "    all_reddit_data = []\n",
    "    for file_path in reddit_jsonl_files:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: Reddit file not found at {file_path}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    all_reddit_data.append(json.loads(line))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    df_reddit = pd.DataFrame(all_reddit_data)\n",
    "\n",
    "    if 'content' not in df_reddit.columns:\n",
    "        print(\"Error: Reddit JSONL files do not contain a 'content' field. Reddit data will be empty.\")\n",
    "        df_reddit = pd.DataFrame(columns=['content', 'sentiment', 'source'])\n",
    "    else:\n",
    "        df_reddit = df_reddit[df_reddit['content'].notna() & (df_reddit['content'].str.strip() != '')].copy()\n",
    "        df_reddit['sentiment'] = pd.NA # Placeholder for Reddit sentiment, will be filled after cleaning\n",
    "        df_reddit['source'] = 'reddit'\n",
    "        df_reddit = df_reddit[['content', 'sentiment', 'source']]\n",
    "        print(f\"Reddit data initial rows: {len(df_reddit)}\")\n",
    "\n",
    "\n",
    "    # --- 3. Combine DataFrames (text and initial labels) ---\n",
    "    print(\"\\n--- Combining DataFrames ---\")\n",
    "    if not df_s140.empty and not df_reddit.empty:\n",
    "        combined_df = pd.concat([df_s140, df_reddit], ignore_index=True)\n",
    "    elif not df_s140.empty:\n",
    "        combined_df = df_s140\n",
    "    elif not df_reddit.empty:\n",
    "        combined_df = df_reddit\n",
    "    else:\n",
    "        combined_df = pd.DataFrame(columns=['content', 'sentiment', 'source'])\n",
    "        print(\"No data was loaded from either source to combine.\")\n",
    "    \n",
    "    if combined_df.empty:\n",
    "        print(\"Combined DataFrame is empty. Exiting processing.\")\n",
    "        return combined_df # Return empty if nothing to process\n",
    "\n",
    "    print(f\"Total rows in combined DataFrame before cleaning: {len(combined_df)}\")\n",
    "\n",
    "    # --- 4. Apply Cleaning to Combined Data ---\n",
    "    print(\"\\n--- Applying Text Cleaning to Combined Data ---\")\n",
    "    combined_df['content'] = combined_df['content'].astype(str)\n",
    "    combined_df['clean_comment'] = combined_df['content'].apply(clean_text)\n",
    "    \n",
    "    # Filter out rows where 'clean_comment' became empty after cleaning\n",
    "    initial_cleaned_rows = len(combined_df)\n",
    "    combined_df = combined_df[combined_df['clean_comment'].str.strip() != ''].copy()\n",
    "    print(f\"Rows after text cleaning: {len(combined_df)} (dropped {initial_cleaned_rows - len(combined_df)} empty clean comments)\")\n",
    "\n",
    "    # --- 5. Apply Sentiment Labeling for Reddit Rows (on cleaned text) ---\n",
    "    print(\"\\n--- Applying Sentiment Labeling for Reddit Data based on cleaned text ---\")\n",
    "    reddit_mask = (combined_df['source'] == 'reddit')\n",
    "    if reddit_mask.any():\n",
    "        combined_df.loc[reddit_mask, 'sentiment'] = combined_df.loc[reddit_mask, 'clean_comment'].apply(label_sentiment_keyword_based)\n",
    "        combined_df['sentiment'] = combined_df['sentiment'].astype(int)\n",
    "        print(f\"Sentiment applied to {reddit_mask.sum()} Reddit rows.\")\n",
    "    else:\n",
    "        print(\"No Reddit data found to label sentiment.\")\n",
    "        \n",
    "    return combined_df[['content', 'clean_comment', 'sentiment']]\n",
    "\n",
    "# --- Define raw data file paths using relative paths ---\n",
    "current_script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "project_root_dir = os.path.dirname(current_script_dir) \n",
    "\n",
    "raw_data_dir = os.path.join(project_root_dir, \"data\", \"raw_data\")\n",
    "output_data_dir = os.path.join(project_root_dir, \"data\")\n",
    "\n",
    "sentiment140_file_path = os.path.join(raw_data_dir, \"Sentiment140.csv\")\n",
    "reddit_jsonl_files = [\n",
    "    os.path.join(raw_data_dir, \"reddit_malaysiauni_20250626_020648.jsonl\"),\n",
    "    os.path.join(raw_data_dir, \"reddit_malaysian_20250626_014418.jsonl\"),\n",
    "    os.path.join(raw_data_dir, \"reddit_malaysianfood_20250621_010756.jsonl\"),\n",
    "    os.path.join(raw_data_dir, \"reddit_malaysian_20250626_013330.jsonl\")\n",
    "]\n",
    "\n",
    "output_cleaned_data_path = os.path.join(output_data_dir, \"cleaned_data.csv\")\n",
    "\n",
    "print(f\"Expected raw data directory: {raw_data_dir}\")\n",
    "print(f\"Expected output directory: {output_data_dir}\")\n",
    "\n",
    "# Process the data\n",
    "final_cleaned_df = process_data_and_combine(sentiment140_file_path, reddit_jsonl_files)\n",
    "\n",
    "# Save the final combined DataFrame to a CSV file\n",
    "if not final_cleaned_df.empty:\n",
    "    try:\n",
    "        final_cleaned_df.to_csv(output_cleaned_data_path, index=False, encoding='utf-8')\n",
    "        print(f\"\\nFinal combined cleaned and labeled data saved to {output_cleaned_data_path}\")\n",
    "        print(f\"Final number of rows in cleaned_data.csv: {len(final_cleaned_df)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving final cleaned data: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo data to save as the final DataFrame is empty.\")\n",
    "\n",
    "print(\"\\nProcessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501dc287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
